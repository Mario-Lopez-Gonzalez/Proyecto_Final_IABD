{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas Rag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Codigo final abajo del todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers faiss-cpu sentence-transformers unstructured\n",
    "# !pip install langchain_community langchain_text_splitters langchain_chroma   #Instalamos todo lo necesario   \n",
    "# !pip install -U langchain-ollama\n",
    "# !pip install -U langchain-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding (investigacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraer info pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "\n",
      "\n",
      " page_content='Contenido  1.  Regresión  3 ¿Cuándo  Utilizar  Regresión?  3 Conceptos  Clave  en  Regresión  3 2.  Tipos  de  Regresión  3 2.1.  Regresión  Lineal  4 2.1.1.  Regresión  Lineal  Simple  4 2.1.2.  Regresión  Lineal  Múltiple  5 2.2.  Regresión  Polinomial  7 2.3.  Regresión  Logística  10 2.4.  Tabla  comparativa  de  Regresiones  10 Resumen  de  las  Diferencias  10 2.5.  Flujo  de  Trabajo  con  Técnicas  de  Optimización  y  Ajuste  en  Regresión  Lineal  11 3.  Métodos  de  optimización  11 Descenso  de  Gradiente:  11 ¿En  Qué  Parte  del  Proceso  se  Usan  los  Métodos  de  Optimización?  15 Resumen  del  Proceso  16 Comparación  entre  tres  métodos  de  optimización  para  resolver  problemas  de  regresión  lineal:  16 4.  Regularización  16 Tipos  de  Regularización  en  Regresión  17 Objetivo  de  la  Regularización  17 4.1.  Regularización  Ridge(L2)  17 Resumen  17 Características:  17 Código  ejemplo  Python:  17 Tabla  comparativa  de  posibles  escenarios  18 Explicación  de  la  Tabla  18 4.2.  Regularización  Lasso(L1)  19 Resumen  19 Características:  19 Código  ejemplo  Python:  19 Tabla  comparativa  de  posibles  escenarios  19 Explicación  de  la  Tabla  Lasso  20 4.3.  Regularización  Elastic  Net  20' metadata={'producer': 'Skia/PDF m135 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': '2_9_RegresionApuntes.docx', 'source': './docs/2_9_RegresionApuntes.pdf', 'total_pages': 30, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"./docs/2_9_RegresionApuntes.pdf\" #Ruta al archivo PDF\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs)) # Imprimimos la cantidad de partes del PDF\n",
    "print('\\n\\n',docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26907/664924813.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2') # modelo de embeddings gratis\n",
      "/home/iabd/anaconda3/envs/IABD3_12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-13 19:56:09.901726: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-13 19:56:09.957301: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-13 19:56:09.971387: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-13 19:56:09.976798: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-13 19:56:10.027082: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/iabd/anaconda3/envs/IABD3_12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2') # modelo de embeddings gratis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50) #usamos menos tokens que en el anterior debido a que este modelo es inferior\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESP:\n",
      " ●  Validación  cruzada:  La  validación  cruzada  es  una  técnica  fundamental  en  modelos  de  regresión  para  evaluar  su  capacidad  de  generalización  y  evitar  problemas  como  el  sobreajuste  o  subajuste.  En  este  contexto,  el  objetivo  es  medir  qué  tan  bien  el  modelo  predice  valores  continuos  en  datos  no  vistos  ●  Tipos  Comunes  de  Validación  Cruzada :  o  K-Fold  Cross-Validation :  \n",
      "▪ \n",
      " \n",
      "Divide\n",
      " \n",
      "los\n",
      " \n",
      "datos\n",
      " \n",
      "en\n",
      " \n",
      "K\n",
      " \n",
      "subconjuntos\n",
      " \n",
      "(folds).\n",
      " \n",
      "▪ \n",
      " \n",
      "Entrena \n",
      "\n",
      "RESP:\n",
      " lineales,  permitiendo  modelar  patrones  más  complejos,  aunque  con  mayor  riesgo  de  sobreajuste  si  el  grado  es  alto. \n",
      "\n",
      "RESP:\n",
      " Puede  parar  el  entrenamiento  en  cuanto  el  error  de  validación  alcanza  el  mínimo  o  parar  solo  después  de  que  el  error  de  validación  lleve  un  tiempo  por  encima  del  mínimo. \n",
      "\n",
      "RESP:\n",
      " de  entrenamiento  y  el  de  validación  son  bajos  y  equilibrados.  o  Acción :  Mantener  el  valor  de  α  o  ajustarlo  ligeramente  si  se  observan  cambios  en  el  error.  ●  Regularización  Moderada-Alta  (Valor  Medio  de  α ) :  o  Con  un  valor  medio  de  α ,  el  modelo  puede  tener  un  ligero  subajuste  si  el  error  es  similar  entre  entrenamiento  y  validación,  pero  un  poco  alto.  o  Acción :  Reducir  α  ligeramente  para  mejorar  el  ajuste.  ● \n",
      "\n"
     ]
    }
   ],
   "source": [
    "resps = retriever.get_relevant_documents('La validación cruzada') \n",
    "for resp in resps:\n",
    "    print('RESP:\\n',resp.page_content,'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bd persistente y llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parte embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48666/3797121527.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2') #modelo de embeddings gratis\n",
      "/home/iabd/anaconda3/envs/IABD3_12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-18 18:19:12.201458: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-18 18:19:12.308363: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-18 18:19:12.339979: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-18 18:19:12.348252: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-18 18:19:12.446626: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/iabd/anaconda3/envs/IABD3_12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2') #modelo de embeddings gratis\n",
    "\n",
    "# documentacion para el rag (pdf)\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"./docs/2_9_RegresionApuntes.pdf\" #Ruta al archivo PDF\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# print(len(docs)) # Imprimimos la cantidad de partes del PDF\n",
    "# print('\\n\\n',docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "persistent_client = chromadb.PersistentClient(path='./bd/vectordb_gratis_prueba')  #Creamos una base de datos para almacenar los vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50) #usamos menos tokens que en el anterior debido a que este modelo es inferior\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings, persist_directory='./bd/vectordb_gratis_prueba')\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46214/4280570917.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retriever.get_relevant_documents('validación cruzada')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='40204e33-9c14-4877-ac38-e7a710ef3d3d', metadata={'creationdate': '', 'creator': 'PyPDF', 'page': 8, 'page_label': '9', 'producer': 'Skia/PDF m135 Google Docs Renderer', 'source': './docs/2_9_RegresionApuntes.pdf', 'title': '2_9_RegresionApuntes.docx', 'total_pages': 30}, page_content='●  Validación  cruzada:  La  validación  cruzada  es  una  técnica  fundamental  en  modelos  de  regresión  para  evaluar  su  capacidad  de  generalización  y  evitar  problemas  como  el  sobreajuste  o  subajuste.  En  este  contexto,  el  objetivo  es  medir  qué  tan  bien  el  modelo  predice  valores  continuos  en  datos  no  vistos  ●  Tipos  Comunes  de  Validación  Cruzada :  o  K-Fold  Cross-Validation :  \\n▪ \\n \\nDivide\\n \\nlos\\n \\ndatos\\n \\nen\\n \\nK\\n \\nsubconjuntos\\n \\n(folds).\\n \\n▪ \\n \\nEntrena'),\n",
       " Document(id='4d143b7e-2235-45fd-b9dd-2f22bc0af6de', metadata={'creationdate': '', 'creator': 'PyPDF', 'page': 8, 'page_label': '9', 'producer': 'Skia/PDF m135 Google Docs Renderer', 'source': './docs/2_9_RegresionApuntes.pdf', 'title': '2_9_RegresionApuntes.docx', 'total_pages': 30}, page_content='●  Validación  cruzada:  La  validación  cruzada  es  una  técnica  fundamental  en  modelos  de  regresión  para  evaluar  su  capacidad  de  generalización  y  evitar  problemas  como  el  sobreajuste  o  subajuste.  En  este  contexto,  el  objetivo  es  medir  qué  tan  bien  el  modelo  predice  valores  continuos  en  datos  no  vistos  ●  Tipos  Comunes  de  Validación  Cruzada :  o  K-Fold  Cross-Validation :  \\n▪ \\n \\nDivide\\n \\nlos\\n \\ndatos\\n \\nen\\n \\nK\\n \\nsubconjuntos\\n \\n(folds).\\n \\n▪ \\n \\nEntrena'),\n",
       " Document(id='671c996d-7016-4711-a453-05092edaaae7', metadata={'creationdate': '', 'creator': 'PyPDF', 'page': 10, 'page_label': '11', 'producer': 'Skia/PDF m135 Google Docs Renderer', 'source': './docs/2_9_RegresionApuntes.pdf', 'title': '2_9_RegresionApuntes.docx', 'total_pages': 30}, page_content='lineales,  permitiendo  modelar  patrones  más  complejos,  aunque  con  mayor  riesgo  de  sobreajuste  si  el  grado  es  alto.'),\n",
       " Document(id='2cd1e6d3-e635-432e-92dd-ce8e8bbbce14', metadata={'creationdate': '', 'creator': 'PyPDF', 'page': 10, 'page_label': '11', 'producer': 'Skia/PDF m135 Google Docs Renderer', 'source': './docs/2_9_RegresionApuntes.pdf', 'title': '2_9_RegresionApuntes.docx', 'total_pages': 30}, page_content='lineales,  permitiendo  modelar  patrones  más  complejos,  aunque  con  mayor  riesgo  de  sobreajuste  si  el  grado  es  alto.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prueba\n",
    "retriever.get_relevant_documents('validación cruzada') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parte llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalar ollama (llm)\n",
    "# !curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# comprobar instalacion\n",
    "# !ollama \n",
    "\n",
    "# instalar modelo \n",
    "# !ollama pull llama3.3    # \"error\":\"model requires more system memory (32.7 GiB) than is available (24.5 GiB)\"}\n",
    "# !ollama pull qwq\n",
    "# !ollama pull deepseek-v2:16b\n",
    "# !ollama pull phi4\n",
    "# !ollama pull gemma3:27b   # \"error\":\"llama runner process has terminated: signal: killed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46610/1633147473.py:7: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model='deepseek-v2:16b', temperature=0)  # [0-1] menor temperatura mas exacto, mayor temperatura mas inventa\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "llm = ChatOllama(model='phi4', temperature=0)  # [0-1] menor temperatura mas exacto, mayor temperatura mas inventa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"Tú eres un asistente para tareas de respuesta a preguntas.\"\n",
    "    \"Usa los siguientes fragmentos de contexto recuperado para responder \"\n",
    "    \"la pregunta. Si no sabes la respuesta, di que no \"\n",
    "    \"sabes. Mantén la respuesta concisa.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag = create_retrieval_chain(retriever, chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " El descenso de gradiente es un método iterativo para minimizar una función de pérdida ajustando los parámetros del modelo, como los coeficientes en regresión lineal. Funciona calculando la dirección del gradiente (derivada parcial) y actualizando los parámetros en la dirección opuesta al gradiente para reducir el error. La idea general es ajustar los parámetros de manera iterativa para minimizar la función de pérdida, comenzando por inicializar θ con valores aleatorios (inicialización aleatoria).\n"
     ]
    }
   ],
   "source": [
    "# Prueba\n",
    "results = rag.invoke({\"input\": \"gradiente descendente\"})\n",
    "# print(results,'\\n\\n')\n",
    "print(results['answer'])  # Respuesta llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codigo final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instalaciones previas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers faiss-cpu sentence-transformers unstructured\n",
    "# !pip install langchain_community langchain_text_splitters langchain_chroma     \n",
    "# !pip install -U langchain-ollama\n",
    "# !pip install -U langchain-huggingface\n",
    "\n",
    "# instalar ollama (llm)\n",
    "# !curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear BD Vectorial Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "# documentacion para el rag (pdf)\n",
    "file_path = \"./docs/2_9_RegresionApuntes.pdf\" # Ruta al archivo PDF\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2') # modelo de embeddings gratis\n",
    "\n",
    "persistent_client = chromadb.PersistentClient(path='./bd/vectordb_gratis_prueba')  # Creamos una base de datos para almacenar los vectores\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50) # usamos menos tokens que en el anterior debido a que este modelo es inferior\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings, persist_directory='./bd/vectordb_gratis_prueba')\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaccion con modelo (preguntas y respuestas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iabd/anaconda3/envs/IABD3_12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cuatro. Tres más uno es igual a cuatro.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "llm = ChatOllama(model='deepseek-v2:16b', temperature=0)\n",
    "chroma_local = Chroma(persist_directory=\"./bd/vectordb_gratis_prueba\", embedding_function=HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2'))\n",
    "    \n",
    "\n",
    "def prompt(texto):\n",
    "    system_prompt = (\n",
    "    texto+\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ])\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def respuesta(pregunta, llm, chroma_db, prompt):\n",
    "    retriever = chroma_db.as_retriever()\n",
    "\n",
    "    chain = create_stuff_documents_chain(llm, prompt)\n",
    "    rag = create_retrieval_chain(retriever, chain)\n",
    "    \n",
    "    resuls = rag.invoke({\"input\": pregunta})\n",
    "    return resuls\n",
    "\n",
    "\n",
    "\n",
    "texto = \"\"\"Tú eres un asistente para tareas de respuesta a preguntas.\"\n",
    "    \"Usa los siguientes fragmentos de contexto recuperado para responder \"\n",
    "    \"la pregunta. Si no sabes la respuesta, di que no \"\n",
    "    \"sabes. Usa un máximo de tres oraciones y mantén la respuesta concisa.\"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    resp = respuesta(input('Formula tu pregunta: '), llm, chroma_local, prompt(texto))\n",
    "    print(resp['answer'])  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IABD3_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
